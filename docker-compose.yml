version: "3.8"

services:
  web_server:
    build:
      dockerfile: ./services/rtc/Dockerfile
    ports:
      - "8088:8088"

  llama:
    build:
      dockerfile: ./services/llama-cpp-python/Dockerfile
    ports:
      - "8089:8000"
    environment:
      USE_MLOCK: 0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  audio_client_parser:
    restart: "on-failure"
    build:
      dockerfile: ./services/client/Dockerfile
    depends_on:
      - web_server
      - transcription_service
    environment:
      URL: web_server:8088
      ROOM: test
      TRANSCRIPTION_SERVICE: http://transcription_service:8000/transcribe
      TRANSLATOR_SERVICE: http://transcription_service:8000/translate
      LLM_SERVICE: http://llama:8000/v1

  transcription_service:
    restart: "on-failure"
    build:
      context: ./services/transcribe
      args:
        MODEL_SIZE: large-v2
    environment:
      MODEL_SIZE: large-v2
      MODEL_DEVICE: cuda
      MODEL_COMPUTE_TYPE: float16
    depends_on:
      - web_server
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
